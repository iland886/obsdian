# 机器学习基础
**分类**
·机器学习的主要任务是分类，利用机器学习算法进行分类，而要用算法来得到目标结果，首先要对算法进行训练
如何用算法分类？
用数据集喂给机器来学习，找到其中的特征信息并做出结果，也称为目标变量，上面提到了机器学习的主要任务是分类，那得到的结果基本上是在已有的特征中归并。

数据集分为训练集和测试集，训练集用来训练算法，测试集来验证算法所得到的目标变量之间的差别，可以的到算法的实际精确值。
****
**回归**
主要是预测数值型数据。

==<u>分类和预测属于监督学习</u>==

**无监督学习**
主要是聚类算法，通常这种数据没有类别信息，也没有目标值。
![[Pasted image 20240614202114.png]]

# K-近邻算法
算法概要：knn的数据集中的每一个数据与所属分类有对应关系。输入新的没有标签的数据。将新数据中的每个特征与数据集中的对应特征进行比较，再从样本集中提取出特征最相似的数据的标签。（一般来说，我们 只提取样本数据前中前k个最相同的数据）

**算法流程**
![[Pasted image 20240616180005.png]]
### kNN算法
```python
def classify01(idX, dataSet, labels,k):  
    dataSetSize = dataSet.shape[0]  
    diffMatrix = np.tile(idX, (dataSetSize,1))- dataSet  
    sqDiffMatrix = np.sum(diffMatrix,axis=1)  
    sqDistance = np.sqrt(sqDiffMatrix)  
    sortedDisIndex = np.argsort(sqDistance)  
    classCount = {}  
    for i in range(k):  
        voteILabel = labels[sortedDisIndex[i]]  
        classCount[voteILabel] = classCount.get(voteILabel,0) +1  
    sortedclassSorted = sorted(classCount.items(), reverse=True)  
    return sortedclassSorted[0][0]  
  
def createDateSet():  
    group = np.array([[1, 1.1], [1,1,],[0,0], [0,0.1]])  
    labels = ['A', 'A', 'B', 'B']  
    return group, labels  
  
  
group, labels = createDateSet()  
clf_res = classify01([0, 0], group, labels, 3)  
print(clf_res)
```
### 数据的标准化
```python
def autoNorm(dataSet):  
  
    dataMin = np.min(dataSet,axis=0)  
    dataMax = np.max(dataSet,axis=0)  
    ranges =  dataMax - dataMin  
    normData = np.zeros_like(dataSet)  
    m = dataSet.shape[0]  
    normData = dataSet - np.tile(dataMax, (m,1))  
    normData = normData /np.tile(ranges, (m,1))  
    return normData, ranges, dataMin
```
# 决策树
流程
![[Pasted image 20240618000612.png]]
信息增益和熵：
·信息增益是在划分数据集前后信息发生的变化
信息：$$l(x_i) = -log_{2}P(x_i)$$
p(xi)是选择该分类的概率
熵：$$H = -\sum_{i=1}^{n}p(x_i)log_{2}P(x_i)$$
n是分类的数目
信息熵的计算 
```python
def clacShannonEnt(dataSet):  
    numEntries = len(dataSet)  
    labelCounts = {}  
    for featVec in dataSet:  
        currentLabel = featVec[-1]  
        if currentLabel not in labelCounts.keys():  
            labelCounts[currentLabel] = 0  
        labelCounts[currentLabel] += 1  
    shannonEnt = 0.0  
    for key in labelCounts:  
        prob = float(labelCounts[key])/numEntries  
        shannonEnt -= np.dot(np.log2(prob), prob)  
    return shannonEnt
```
熵越高，则混合的数据越多

**如何划分数据集**
```python
def splitDateSet(dataSet, axis, value):  
    retDateSet = []  
    for featVec in dataSet:  
        if featVec[axis] == value:  
            reduceFeatVec = featVec[:axis]  
            reduceFeatVec.extend(featVec[axis+1:])  
            retDateSet.append(reduceFeatVec)  
    return retDateSet
    
```
## 朴素贝叶斯
![[Pasted image 20240626101749.png]]
一般过程
![[Pasted image 20240626102431.png]]
朴素的意思是每个特征同等重要。
