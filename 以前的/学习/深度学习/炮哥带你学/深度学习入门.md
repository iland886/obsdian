## 全连接神经网络
#激活函数
为什么加入激活函数
```markdown
一个神经网络框架：
输入层--> 隐藏层 --（激活函数）--> 输出层

激活函数是非线性的， 如果是线性的，在最后一层前的激活函数运算可以用最后一层的激活函数表示h(x) = cx, 经过n-1 层隐藏层的激活函数y(x)= c^(n-1)x，最后一层只需要以G(x)=c^(n-1)x来处理，不需要隐藏层。

```
激活函数分类：
1. Sigmoid函数
	优点：简单，适用于分类任务
	缺点：反向传播训练有梯度消失问题（SigMoid导数太大）
		输出值区间为（0,1）关于0不对称，梯度更新在不同方向走的太远，训练费事

	Tanh函数：
	优点：解决了Sigmoid函数值非0对称的问题，训练快，更容易收敛
	缺点：存在梯度消失问题

	ReLu函数
	优点：解决了梯度消失问题，计算更简单
	缺点：训练时肯呢个会导致神经元死亡

	Leakly reLu函数
	优点：解决了ReLu函数神经元死亡的问题
	缺点：无法为正负输入值提供已知的关系预测（不同区间函数）

	SoftMax函数
	用于多分类任务

#前向传播
