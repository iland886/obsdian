1.深度学习中的激活函数需要不具备下面哪项属性？
	激活函数是什么？
		一种非线性的数学函数，能够让网络学习复杂的非线性模式，解决线性模型无法处理的问题
		提高模型的表达能力，一般来说参数的规模越大，模型的复杂程度越高，非线性能力越强，**避免梯消失或爆炸**

2.神经网络优化器
常用的有sgd，RMSprop，adam。
如何比较优化器的训练效果？
	收敛效果（即最终达到的损失值或模型性能）取决于任务、数据集、模型架构和超参数设置。
	不能单一的看某个模型的优点。

| 优化器        | SGD                                                                                               | RMSprop                                                                                                                                                                 | Adam                                                                                                                                                                                                                       |
| ---------- | ------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **是什么**    | 随机梯度下降，使用小批量数据梯度更新参数，可结合动量法。<br>更新公式：<br>\(\theta = \theta - \eta \cdot \nabla_\theta J(\theta)\) | 自适应学习率优化器，基于梯度平方指数移动平均调整学习率。<br>更新公式：<br>\(E[g^2]_t = \rho E[g^2]_{t-1} + (1-\rho) g_t^2\)<br>\(\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_t\) | 结合动量法和RMSprop，基于一阶和二阶动量自适应更新。<br>更新公式：<br>\(m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t\)<br>\(v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2\)<br>\(\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t\) |
| **为什么**    | 简单、理论扎实，随机采样降低计算成本，噪声有助于逃离局部最优。                                                                   | 解决SGD学习率难选问题，自适应调整适合非平稳目标函数。                                                                                                                                            | 综合动量和自适应学习率，收敛快且对超参数鲁棒。                                                                                                                                                                                                    |
| **有什么用**   | 适合大规模数据集，理论上可收敛到全局最优，需精细调参。                                                                       | 适合非平稳问题（如RNN），收敛速度快，减少调参负担。                                                                                                                                             | 适合快速实验、轻量级网络和通用任务，收敛稳定高效。                                                                                                                                                                                                  |
| **收敛速度**   | 较慢                                                                                                | 较快                                                                                                                                                                      | 快                                                                                                                                                                                                                          |
| **超参数敏感性** | 高（学习率、动量需手动调参）                                                                                    | 中等（衰减率等需调整）                                                                                                                                                             | 低（默认超参数通常有效）                                                                                                                                                                                                               |
| **适用场景**   | 大规模数据集，需高精度收敛的任务                                                                                  | 非平稳问题（如RNN）                                                                                                                                                             | 轻量级网络，快速实验，通用任务                                                                                                                                                                                                            |
| **理论保证**   | 强（凸问题可收敛到全局最优）                                                                                    | 中等（无强理论保证）                                                                                                                                                              | 弱（可能收敛到次优解）                                                                                                                                                                                                                |
| **优点**     | 实现简单，适合大规模数据，理论支持强                                                                                | 自适应学习率，收敛较快，适合非平稳问题                                                                                                                                                     | 收敛快，超参数鲁棒，适合大多数场景                                                                                                                                                                                                          |
| **缺点**     | 收敛慢，易受学习率影响，易陷入局部最优或鞍点                                                                            | 超参数敏感，可能收敛到次优解                                                                                                                                                          | 稀疏数据可能次优，理论保证弱                                                                                                                                                                                                             |

- **共同目标**：通过梯度信息最小化损失函数，优化神经网络参数。
- **梯度基础**：基于梯度下降原理，依赖小批量数据随机梯度。
- **发展脉络**：
  - SGD：最基础的优化器。
  - RMSprop：改进SGD，引入自适应学习率。
  - Adam：结合RMSprop和动量法，提升收敛速度和稳定性。
- **超参数依赖**：均需设置学习率，RMSprop和Adam额外需衰减率等参数。
- **适用性**：效果依赖任务、数据集和模型架构，常需实验比较。

## 总结

- **SGD**：适合需高精度的大规模任务，收敛慢但理论强。
- **RMSprop**：适合非平稳问题，收敛较快但超参数敏感。
- **Adam**：收敛快、鲁棒性强，适合轻量级网络和快速实验。
- **选择建议**：轻量级网络选Adam，RNN选RMSprop，大规模或高精度任务选SGD（带动量）。

3、以下说法错误的是（   A   ）

A.使用ReLU做为激活函数，可有效地防止梯度爆炸        relu函数在大于0的时候输出原值，不会因为多次乘积而指数级放大（sigmoid的倒数<1 会造成梯度消失）ReLU不能完全防止梯度爆炸, 只能缓解梯度消失问题

B.使用Sigmoid做为激活函数，较容易出现梯度消失  $f(x) =\frac{1}{1+e^{-x}}$ 输出范围是(0,1)，梯度消失是指在深层网络中梯度因乘积而变得极小，导致参数更新失败。

C.使用Batch Normalization层，可有效的防止梯度爆炸    BN通过归一化输入，限制激活值的范围，间接控制梯度大小。

D.使用参数weight decay，在一程度上可防止模型过拟合  一种正则化技术，在损失函数中添加权重范数的惩罚项（如L2范数），形式为$L=L_{original}+λ∑w^{2}_{i}$ ,λ是正则化系数.


4. LSTM（长短时记忆网络）是RNN的一种变体，通过引入遗忘门、输入门和输出门来控制信息流动。当梯度多次相乘消失或爆炸，LSTM通过门控机制选择保留或者以往信息看，缓解梯度消失和爆炸。
5. CNN（卷积神经网络）通过卷积和池化层提取局部特征，全连接网络（FCN）则每个神经元与上一层全部连接。CNN的结构确实降低复杂度并缓解过拟合

6. 梯度下降算法的正确步骤：
	初始化权重和偏差
	将输入传入网络，得到输出值
	计算预测值和真实值之间的误差
	对每一个产生误差的神经元，调整相应的权重值以减小误差
	重复迭代，直至得到网络权重的最佳值

8.**感受野**：是指输入图像中，某个特定神经元在输出层所能“看到”的区域大小.
计算感受野：如果对一层，卷积核为k，步长为s，前一层感受野大小r_in,前一层步长为j_in，该层感受野大小和间隔的计算公式：$r_{out} =r_{in} +(k - 1) \ j_{in}$     $j_{out} = j_{in} s$

9.如何增强模型能力？
增加隐藏层数，加深网络，但可能会增加过拟合风险或训练难度
Dropout是一项正则化技术，用来降低模型能力的，防止过拟合
学习率是参数更新的步长，影响收敛速度和稳定性，影响训练效果，不直接增加模型能力，学习率过大会导致损失函数难以稳定在最优值。


10.批规范化(Batch Normalization)的好处
BN通过标准化将每一层输入的均值调整为0， 方差调整为1
核心功能之一就是让输入范围大致固定

BN作用域层的输入，而不是权重本身，要用到另一个技术权重归一化(weight normalization)

11.激活函数有哪几种，写出公式和代码
Sigmoid函数 $f(x) = \frac{1}{1 + e^{-x}}$
Tanh激活函数$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
Relu函数 relu(x) = max(0, x)
Leaky ReLU f(x) = max(ax, x)
softmax(x) $f(x) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}$ 
```python
import torch.nn,functional as F
import torch.nn as nn
F.sigmoid(x)
F.tanh(x) or torch.tanh(x)
F.relu(x)
nn.leakly_relu(x)
F.softmax(x)
```

12.损失函数
mse均方误差 $l = \frac{1}{n}\sum_{i=1}^{n}(y_i - \dot y_i)^2$  
MAE均绝对误差 l = $\frac{1}{n}\sum_{i=1}^{n}|y_i - \dot y_i|$ 
CrossEntropyLoss交叉损失熵 $- \frac{1}{n}\sum_{i = 1}^{n}[y_ilog(y_i) + (1 - y_i)log(1- y_i)]$
KL 散度损失 (KL Divergence Loss) $L = \sum_{i}[p_ilog(p_i)-p_ilog(q_i)]$
hige loss  $L = max(0, 1 - y_i \dot y_i)$  

13.优化器
sgd $\theta_{t+1}  = \theta_t - \beta j$ 
Adam
动量法 (Momentum)
RMSprop
Adagrad
```python
# sgd
lr = 0.01  # 学习率
theta = theta - lr * grad

#momentum动量法
v = mu * v - lr * grad
theta = theta + v


#rmsprop
Eg = gamma * Eg + (1 - gamma) * grad**2
theta = theta - lr / (Eg.sqrt() + epsilon) * grad

#adagrad
G = G + grad ** 2
theta = theta - lr / (G.sqrt() + epsilon) * grad
#adam
t += 1
m = beta1 * m + (1 - beta1) * grad
v = beta2 * v + (1 - beta2) * grad ** 2

m_hat = m / (1 - beta1 ** t)
v_hat = v / (1 - beta2 ** t)

theta = theta - lr * m_hat / (v_hat.sqrt() + epsilon)

```

14.梯度消失，梯度爆炸，在反向传播过程中，梯度的乘积随着层数增加而小于1或称指数级爆炸导致网络参数更新过小，训练不够，或参数更新过大，训练发散


如何缓解？
缓解梯度消失的方法，使用relu函数或者relu_leakly函数,使用批量归一化(batch normalization),残差连接

缓解梯度爆炸的方法，梯度裁剪，批量归一化，降低学习率

15.什么是生成对抗网络？
GAN 是一种由生成网络 (Generator) 和判别网络 (Discriminator) 组成的深度学习框架。生成网络从随机噪声生成假数据，判别网络区分真假数据，两个网络通过对抗训练共同优化。

GAN, DCGAN, CGAN,WGAN, cycleGAN


16.vgg11网络构建
```python
import torch
import torch.nn as nn

class VGG_A(nn.Module):
    def __init__(self, num_classes=1000):
        super(VGG_A, self).__init__()
        self.features = nn.Sequential(
            # conv3-64
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # conv3-128
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # conv3-256 x2
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # conv3-512 x2
            nn.Conv2d(256, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),

            # conv3-512 x2
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        self.classifier = nn.Sequential(
            nn.Linear(512 * 7 * 7, 4096),  # 因为输入是224x224，五次pooling后是7x7
            nn.ReLU(inplace=True),
            nn.Dropout(),

            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),

            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)  # flatten all dimensions except batch
        x = self.classifier(x)
        return x

```