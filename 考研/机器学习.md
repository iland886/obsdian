### 决策树
1. 简介
	是一种监督学习模型，用于回归和分类.
	树的结构：
		根节点
		内部节点
		叶节点
		分裂
	工作原理：
		·	从根节点开始，基于某个特征将数据分裂为子节点。
		· 递归的对每个子节点分裂，知道满足停止条件
		·分裂时从根节点开始沿着树路径达到叶节点，并输出叶节点的类别
	关键问题：
		·特征选择
		·分裂标准
		·停止条件
		·剪枝
2. 核心算法
·特征选择和分裂标准
	依靠信息增益，增益率，基尼指数选择最佳特征进行分裂。
	(1) 信息熵
		衡量数据集的混乱程度，对数据集D，类别为K，每类概率为$p_i$, 熵定义为
		$$Entropy(D) = -\sum_{i=1}^{K}p_ilogp_i$$
		其中$p_i = \frac{C_i}{D}$ ,熵越小，表示数据纯度越高（样本更偏向同一类）
	（2）信息增益（ID3）
		信息增益是衡量特征A对数据集的纯度提升
		$$Gain(D, A) = Entropy(D) - \sum_{v=1}^{V}\frac{D_v}{D}Entropy(D_V)$$
		$D_v$是特征A取值的子集，V是特征A可能得取值，减号后者也称为**条件熵** 。
		缺点：偏向选择取值较多的特征。
	（3）增益率
	$$GainRatio(D, A) = \frac{Gain(D, A)}{IV(A)}$$
	    C4.5选择增益率最大的特征。
	   (4) 基尼系数（Gini Index)
	   $$Gini(D) = 1 - \sum^{k}_{i=1}p_i^2$$
	   ·gini系数越小，表示数据集纯度越高，用来衡量数据集的不纯度
	   特征A的基尼增益
	   $$GiniGain(D, A) = Gini(D) - \sum_{v=1}^{V}\frac{D_v}{D}Gini(D_v)$$
	   ·CART选择基尼增益最大的特征
·停止条件
	   1. 节点中样本属于同一类
	   2. 样本数少于最小阈值
	   3. 树的深度达到最大值
	   4. 信息增益、增益率、基尼系数小于阈值

·剪枝
	预剪枝
	后剪枝
	C4.5和CART常使用后剪枝，ID3通常不剪枝
3. 算法代码
输入：训练数据集$D=\{(x_i,y_i)\}^{N}_{i=1}$        特征集$A = \{A_1, A2, ..., A_m\}$

```text
算法 DecisionTree(D, A)
    // 创建节点
    node ← 新节点

    // 终止条件
    如果 D 中样本全属于同一类别 C:
        将 node 标记为类别 C 的叶节点
        返回 node
    如果 A 为空 或 D 中样本在 A 上取值相同:
        将 node 标记为 D 中样本最多的类别
        返回 node
    如果 样本数 < 最小阈值 或 树深度 ≥ 最大深度:
        将 node 标记为 D 中样本最多的类别
        返回 node

    // 选择最佳特征
    best_feature ← None
    max_gain ← -∞
    对于每个特征 A_i ∈ A:
        gain ← ComputeInformationGain(D, A_i)
        如果 gain > max_gain:
            max_gain ← gain
            best_feature ← A_i

    // 分裂
    将 node 标记为特征 best_feature
    对于 best_feature 的每个取值 v:
        D_v ← D 中满足 best_feature = v 的子集
        如果 D_v 为空:
            创建叶节点，标记为 D 中样本最多的类别
            将叶节点加入 node 的子节点
        否则:
            子树 ← DecisionTree(D_v, A \ {best_feature})
            将子树加入 node 的子节点

    返回 node
结束算法

函数 ComputeInformationGain(D, A_i)
    entropy_D ← Entropy(D)
    entropy_split ← 0
    对于 A_i 的每个取值 v:
        D_v ← D 中满足 A_i = v 的子集
        entropy_split ← entropy_split + (|D_v| / |D|) * Entropy(D_v)
    返回 entropy_D - entropy_split
结束函数
```
4. 时间复杂度：
	不剪枝：（树的深度近似n）T(n)=O($mn^2$ )
	剪枝：（$树的深度近似 logn  $\\ T(n) = O(mnlogn)$ 
5. 模型的优化
  剪枝操作
	·预剪枝
		进行递归前设置递归终止条件
		优点：计算效率高，可以提前停止分裂，减少构建时间
		缺点：容易欠拟合，难以确定阈值
	·后剪枝
		让树完全生长，在往顶点回溯的时候，移除那些对泛化性能贡献不大的子树。
		定义代价复杂性剪枝，引入复杂度乘法参数$\alpha$ ,定义损失函数：$$R_\alpha(T) = R(T) + \alpha\cdot|T|$$
		   ·R(T)：树的误分裂误差（或MSE）
		   ·T：树的叶节点树
		   ·$\alpha$ ：惩罚系数
		优点：允许树优先生长，泛化能力通常优于预剪枝，更灵活，利于数据的动态调整。
			缺点：计算量大，先构建完整个树，在回溯剪枝，需要验证集
### 支持向量机
#### 感知机模型
1. 感知机模型
	最基础的二分类模型之一，属于监督学习算法，用于处理线性可分的数据。

	原理概述：
	 感知机是一个线性二分类器，用于将输入数据划分为两个类别，通过学习一个线性超平面，将数据点分隔开。形式为：$$f(x) = sign(wx+b) = 
	\begin{cases}
	+1  \\
	-1
	\end{cases}
	$$
		·x为输入的特征向量
		·w为权重向量，决定超平面的方向
		·b为偏置，决定超平面的位置
	
	1. 感知机的原理：
		找到一个超平面使得所有正类点在一侧，父类点在另一侧。
		·若数据可分，感知机迭代更新权重得到最优的w, b找到超平面
		·若数据不可分, 感知机可能无法收敛
	
	2. 感知机的算法流程：
		基于梯度下降算法，通过最小化误分类点的损失来优化w，b
		输入：数据集$D=\{ (x_i, y_i) \}^N$ 
		1.定义损失函数$$L(w,b) = -\sum^{}_{x_i\in M }y_i(wx_i+b)$$
			·M为误分类点集合
			·最小化损失函数，使所有点都被分类正确
		2.对权重进行更新
		·初始化w = b = 0， $\eta  > 0$    
		·对数据集D中样本点进行遍历，若存在误分类 即$y_i(wx_i+b)$ < 0, 更新参数$$\begin{cases}
		    w = w +  y_i\cdot x_i\cdot \eta \\
		    b = b +  y_i\cdot \eta 
		\end{cases}$$
		·重复直到没有误分类点或到最大迭代次数(参数的更新是基于对参数求偏导)
		输出 ：w,b
		伪代码如下
		```text
		输入: 数据集 D = {(x_i, y_i)}, 学习率 η
		初始化: w = 0, b = 0
		while 存在误分类点:
		    for each (x_i, y_i) in D:
		        if y_i * (w · x_i + b) ≤ 0:
		            w ← w + η * y_i * x_i
		            b ← b + η * y_i
		输出: w, b
		```
		时间复杂度：n个样本(O(n))，计算内积(O(m)),总计（O(mn)）
	
	3. 感知机的对偶形式
		与原始形式相比，它引入Gram矩阵和对偶变量来表达模型参数，从而减少计算量。
	
		因为一个样本点$x_i$可能会被分类$n_i次，总更新次数为\sum_in_i$ ,权重w是所有更新累加的结果$$w = \sum_{i=1}^{n}n_i\cdot x_i\cdot y_i \cdot \eta$$ 定义对偶变量$\alpha = n_i\eta_i$ ,表示$x_i$的累积贡献,同理b也是，最后写成$$\begin{cases}
		w = \sum_{i=1}^{N} \alpha_ix_iy_i \\
		b = \sum_{i=1}^{N} \alpha_iy_i
		\end{cases}$$
		对偶形式的分类函数$$f(x) = sign(\sum_{i=1}^{N}\alpha_iy_ix_i\cdot x + b)$$
		·$其中,x_i\cdot x$是样本的内积，构成Gram矩阵
		·G = $[x_i\cdot x_j]_{nxn},G_{ij} = x_i\cdot x_j$ 
	
	是样本间的内积$	
		对偶模式也从更新w,b到$\alpha, b$  
		伪代码：
	```pseudo
	输入: 数据集 D = {(x_i, y_i)}, 学习率 η
	初始化: α_i = 0 (i=1,...,n), b = 0
	计算 Gram 矩阵 G_ij = x_i · x_j
	while 存在误分类点:
	    for each (x_i, y_i) in D:
	        if y_i * (∑_{j=1}^n α_j y_j G_ji + b) ≤ 0:
	            α_i ← α_i + η
	            b ← b + η y_i
	输出: α, b
	```
2. 支持向量机
	支持向量机是一种监督学习算法，主要用于二分类，它是学习一个找到一个最大间隔超平面来分离两类数据。SVM的主要思想是最大化分类边界。

	SVM的工作原理：
	根据数据是否线性可分，分为 硬间隔SVM，软间隔SVM和核SVM。
	1. 硬间隔SVM
	   ·目标：找到一个超平面wx+b=0，最大化间隔。
	   ·数据集D
	   ·约束：所有样本分类正确，$y_i(wx_i+b) \geq 1$   
	   ·间隔：超平面到支持向量的距离为$\frac{1}{|W|},总间隔\frac{2}{|w|}$ 
	   ·优化问题：$$min_{w,b}\frac{1}{2}||W||^2 \\ {}$$$$s.t. y_i(wx_i+b) \geq 1$$·最小化问题等价于最大化$\frac{2}{|W|}$ 
	2. 软间隔SVM(线性不可分)
	      数据集包含噪声，硬间隔可能不存在。
	    ·引入松弛变量$\delta_i\geq 0$ 允许某些样本误分类或靠近超平面$$y_i(wx_i+b) \geq 1 - \delta_i$$·优化问题：$$\begin{align*}&min_{w,b}\frac{1}{2}||W||^2 + C\sum_{i=1}^{n}\delta_i \\&s.t. y_i(wx_i+b) \geq 1\end{align*}$$·C：惩罚参数，控制间隔最大化与误分类惩罚的平衡
	3. 核 SVM（非线性可分）
	   数据非线性可分时，SVM 通过核技巧将数据映射到高维空间，使其线性可分。
	   核函数：$k(x_i,x_j) = \Phi(x_i)\cdot \Phi(x_j)$   
	   常用核函数：线性核，多项式核，高斯核
	SVM的对偶形式
	（1）硬间隔SVM的优化
		
	















































