### 决策树
1.简介
	是一种监督学习模型，用于回归和分类.
	树的结构：
		根节点
		内部节点
		叶节点
		分裂
	工作原理：
		·	从根节点开始，基于某个特征将数据分裂为子节点。
		· 递归的对每个子节点分裂，知道满足停止条件
		·分裂时从根节点开始沿着树路径达到叶节点，并输出叶节点的类别
	关键问题：
		·特征选择
		·分裂标准
		·停止条件
		·剪枝
2.核心算法
·特征选择和分裂标准
	依靠信息增益，增益率，基尼指数选择最佳特征进行分裂。
	(1) 信息熵
		衡量数据集的混乱程度，对数据集D，类别为K，每类概率为$p_i$, 熵定义为
		$$Entropy(D) = -\sum_{i=1}^{K}p_ilogp_i$$
		其中$p_i = \frac{C_i}{D}$ ,熵越小，表示数据纯度越高（样本更偏向同一类）
	（2）信息增益（ID3）
		信息增益是衡量特征A对数据集的纯度提升
		$$Gain(D, A) = Entropy(D) - \sum_{v=1}^{V}\frac{D_v}{D}Entropy(D_V)$$
		$D_v$是特征A取值的子集，V是特征A可能得取值，减号后者也称为**条件熵** 。
		缺点：偏向选择取值较多的特征。
	（3）增益率
	$$GainRatio(D, A) = \frac{Gain(D, A)}{IV(A)}$$
	    C4.5选择增益率最大的特征。
	   (4) 基尼系数（Gini Index)
	   $$Gini(D) = 1 - \sum^{k}_{i=1}p_i^2$$
	   ·gini系数越小，表示数据集纯度越高，用来衡量数据集的不纯度
	   特征A的基尼增益
	   $$GiniGain(D, A) = Gini(D) - \sum_{v=1}^{V}\frac{D_v}{D}Gini(D_v)$$
	   ·CART选择基尼增益最大的特征
·停止条件
	   1. 节点中样本属于同一类
	   2. 样本数少于最小阈值
	   3. 树的深度达到最大值
	   4. 信息增益、增益率、基尼系数小于阈值

·剪枝
	预剪枝
	后剪枝
	C4.5和CART常使用后剪枝，ID3通常不剪枝
3.算法代码
输入：训练数据集$D=\{(x_i,y_i)\}^{N}_{i=1}$        特征集$A = \{A_1, A2, ..., A_m\}$

```text
算法 DecisionTree(D, A)
    // 创建节点
    node ← 新节点

    // 终止条件
    如果 D 中样本全属于同一类别 C:
        将 node 标记为类别 C 的叶节点
        返回 node
    如果 A 为空 或 D 中样本在 A 上取值相同:
        将 node 标记为 D 中样本最多的类别
        返回 node
    如果 样本数 < 最小阈值 或 树深度 ≥ 最大深度:
        将 node 标记为 D 中样本最多的类别
        返回 node

    // 选择最佳特征
    best_feature ← None
    max_gain ← -∞
    对于每个特征 A_i ∈ A:
        gain ← ComputeInformationGain(D, A_i)
        如果 gain > max_gain:
            max_gain ← gain
            best_feature ← A_i

    // 分裂
    将 node 标记为特征 best_feature
    对于 best_feature 的每个取值 v:
        D_v ← D 中满足 best_feature = v 的子集
        如果 D_v 为空:
            创建叶节点，标记为 D 中样本最多的类别
            将叶节点加入 node 的子节点
        否则:
            子树 ← DecisionTree(D_v, A \ {best_feature})
            将子树加入 node 的子节点

    返回 node
结束算法

函数 ComputeInformationGain(D, A_i)
    entropy_D ← Entropy(D)
    entropy_split ← 0
    对于 A_i 的每个取值 v:
        D_v ← D 中满足 A_i = v 的子集
        entropy_split ← entropy_split + (|D_v| / |D|) * Entropy(D_v)
    返回 entropy_D - entropy_split
结束函数
```
4.模型的优化
1.  剪枝操作
	·预剪枝
		进行递归前设置递归终止条件
		优点：计算效率高，可以提前停止分裂，减少构建时间
		缺点：容易欠拟合，难以确定阈值
	·后剪枝
		让树完全生长，在往顶点回溯的时候，移除那些对泛化性能贡献不大的子树。
		定义代价复杂性剪枝，引入复杂度乘法参数$\alpha$ ,定义损失函数：$$R_\alpha(T) = R(T) + \alpha\cdot|T|$$
		   ·R(T)：树的误分裂误差（或MSE）
		   ·T：树的叶节点树
		   ·$\alpha$ ：惩罚系数
		优点：允许树优先生长，泛化能力通常优于预剪枝，更灵活，利于数据的动态调整。
			缺点：计算量大，先构建完整个树，在回溯剪枝，需要验证集
### 支持向量机
